{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LeNet Class\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import optimizers\n",
    "from chainer import Variable\n",
    "from chainer import cuda\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LeNet(chainer.Chain):\n",
    "    def __init__(self, num_class, train=True):\n",
    "        super(LeNet, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1=L.Convolution2D(None, 6, 5, stride=1)\n",
    "            self.conv2=L.Convolution2D(None, 16, 5, stride=1)\n",
    "            self.conv3=L.Convolution2D(None, 120, 5, stride=1)\n",
    "            self.fc4=L.Linear(None, 84)\n",
    "            self.fc5=L.Linear(None, num_class)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        h1 = F.max_pooling_2d(F.local_response_normalization(\n",
    "            F.sigmoid(self.conv1(x))), 2, stride=2)\n",
    "        h2 = F.max_pooling_2d(F.local_response_normalization(\n",
    "            F.sigmoid(self.conv2(h1))), 2, stride=2)\n",
    "        h3 = F.sigmoid(self.conv3(h2))\n",
    "        h4 = F.sigmoid(self.fc4(h3))\n",
    "        h5 = self.fc5(h4)\n",
    "        \n",
    "        return h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNIST data set\n",
    "\n",
    "train, test = chainer.datasets.get_mnist()\n",
    "_xs, ts = train._datasets\n",
    "_txs, tts = test._datasets\n",
    "\n",
    "size = 5000\n",
    "_xs = _xs[:size]\n",
    "ts = ts[:size]\n",
    "_txs = _txs[:size]\n",
    "tts = tts[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# padding (60000, 784) -> (60000, 1, 28, 28) -> (60000, 1, 32, 32)\n",
    "\n",
    "_v0 = np.row_stack((np.zeros(28), np.zeros(28)))\n",
    "v0 = np.array(_v0)\n",
    "_h0 = np.column_stack((np.zeros(32), np.zeros(32)))\n",
    "h0 = np.array(_h0)\n",
    "\n",
    "def padding(x):\n",
    "    tmp1 = np.vstack((x, v0))\n",
    "    tmp2 = np.vstack((v0, tmp1))\n",
    "    _tmp1 = np.hstack((tmp2, h0))\n",
    "    _tmp2 = np.hstack((h0, _tmp1))\n",
    "    return _tmp2\n",
    "\n",
    "xs_list = []\n",
    "for i in range(len(_xs)):\n",
    "    x = np.reshape(_xs[i], (28, 28))\n",
    "    pad_x = padding(x)\n",
    "    xs_list.append(pad_x[np.newaxis, :, :])\n",
    "txs_list = []\n",
    "for i in range(len(_txs)):\n",
    "    tx = np.reshape(_txs[i], (28, 28))\n",
    "    pad_tx = padding(tx)\n",
    "    txs_list.append(pad_tx[np.newaxis, :, :])\n",
    "    \n",
    "xs = np.array(xs_list, dtype=np.float32)\n",
    "txs = np.array(txs_list, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# method\n",
    "\n",
    "def check_accuracy(model, xs, ts, batchsize):\n",
    "    loss = 0\n",
    "    num_cors = 0\n",
    "    for i in range(0, len(xs), batchsize):\n",
    "        x = xs[i:i + batchsize]\n",
    "        t = ts[i:i + batchsize]\n",
    "        \n",
    "        var_xs = Variable(cuda.to_gpu(x))\n",
    "        #var_xs = Variable(x)\n",
    "        t = Variable(cuda.to_gpu(np.array(t, \"i\")))\n",
    "        #t = Variable(np.array(t, \"i\"))\n",
    "        ys = model(var_xs)\n",
    "    \n",
    "        loss += F.softmax_cross_entropy(ys, t)\n",
    "        ys = np.argmax(ys.data, axis=1)\n",
    "        _t = cuda.to_gpu(np.array(cuda.to_cpu(t.data), dtype=np.float32))\n",
    "        #_t = np.array(t.data, dtype=np.float32)\n",
    "        cors = (ys == _t)\n",
    "        num_cors += sum(cors)\n",
    "    accuracy = num_cors / ts.shape[0]\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss(train) = variable(115.07794189453125), accuracy(train) = 0.11, accuracy(test) = 0.1024\n",
      "Epoch 2 loss(train) = variable(115.02818298339844), accuracy(train) = 0.11, accuracy(test) = 0.1024\n",
      "Epoch 3 loss(train) = variable(114.81937408447266), accuracy(train) = 0.11, accuracy(test) = 0.1024\n",
      "Epoch 4 loss(train) = variable(113.13392639160156), accuracy(train) = 0.219, accuracy(test) = 0.2092\n",
      "Epoch 5 loss(train) = variable(96.60061645507812), accuracy(train) = 0.5112, accuracy(test) = 0.4562\n",
      "Epoch 6 loss(train) = variable(66.64697265625), accuracy(train) = 0.6408, accuracy(test) = 0.5922\n",
      "Epoch 7 loss(train) = variable(48.60980224609375), accuracy(train) = 0.749, accuracy(test) = 0.7022\n",
      "Epoch 8 loss(train) = variable(38.69767379760742), accuracy(train) = 0.7952, accuracy(test) = 0.7478\n",
      "Epoch 9 loss(train) = variable(32.431636810302734), accuracy(train) = 0.8244, accuracy(test) = 0.7774\n",
      "Epoch 10 loss(train) = variable(27.980743408203125), accuracy(train) = 0.8428, accuracy(test) = 0.797\n",
      "Epoch 11 loss(train) = variable(24.60479164123535), accuracy(train) = 0.8616, accuracy(test) = 0.8156\n",
      "Epoch 12 loss(train) = variable(21.938709259033203), accuracy(train) = 0.8766, accuracy(test) = 0.833\n",
      "Epoch 13 loss(train) = variable(19.821142196655273), accuracy(train) = 0.8906, accuracy(test) = 0.845\n",
      "Epoch 14 loss(train) = variable(18.13141441345215), accuracy(train) = 0.8974, accuracy(test) = 0.858\n",
      "Epoch 15 loss(train) = variable(16.747835159301758), accuracy(train) = 0.904, accuracy(test) = 0.866\n",
      "Epoch 16 loss(train) = variable(15.581657409667969), accuracy(train) = 0.9104, accuracy(test) = 0.872\n",
      "Epoch 17 loss(train) = variable(14.56933879852295), accuracy(train) = 0.9162, accuracy(test) = 0.879\n",
      "Epoch 18 loss(train) = variable(13.671870231628418), accuracy(train) = 0.9206, accuracy(test) = 0.8862\n",
      "Epoch 19 loss(train) = variable(12.864673614501953), accuracy(train) = 0.9256, accuracy(test) = 0.8904\n",
      "Epoch 20 loss(train) = variable(12.131863594055176), accuracy(train) = 0.9296, accuracy(test) = 0.8952\n",
      "Epoch 21 loss(train) = variable(11.462515830993652), accuracy(train) = 0.9354, accuracy(test) = 0.9\n",
      "Epoch 22 loss(train) = variable(10.84732723236084), accuracy(train) = 0.9394, accuracy(test) = 0.9036\n",
      "Epoch 23 loss(train) = variable(10.282024383544922), accuracy(train) = 0.943, accuracy(test) = 0.9066\n",
      "Epoch 24 loss(train) = variable(9.76430606842041), accuracy(train) = 0.9458, accuracy(test) = 0.909\n",
      "Epoch 25 loss(train) = variable(9.290099143981934), accuracy(train) = 0.9488, accuracy(test) = 0.9112\n",
      "Epoch 26 loss(train) = variable(8.8592529296875), accuracy(train) = 0.9512, accuracy(test) = 0.9148\n",
      "Epoch 27 loss(train) = variable(8.466462135314941), accuracy(train) = 0.9538, accuracy(test) = 0.9174\n",
      "Epoch 28 loss(train) = variable(8.109615325927734), accuracy(train) = 0.9552, accuracy(test) = 0.9204\n",
      "Epoch 29 loss(train) = variable(7.784205436706543), accuracy(train) = 0.9576, accuracy(test) = 0.9222\n",
      "Epoch 30 loss(train) = variable(7.486433029174805), accuracy(train) = 0.9594, accuracy(test) = 0.9232\n",
      "Epoch 31 loss(train) = variable(7.2137651443481445), accuracy(train) = 0.9608, accuracy(test) = 0.925\n",
      "Epoch 32 loss(train) = variable(6.964099407196045), accuracy(train) = 0.9634, accuracy(test) = 0.9274\n",
      "Epoch 33 loss(train) = variable(6.73442268371582), accuracy(train) = 0.9648, accuracy(test) = 0.928\n",
      "Epoch 34 loss(train) = variable(6.521526336669922), accuracy(train) = 0.966, accuracy(test) = 0.929\n",
      "Epoch 35 loss(train) = variable(6.323784351348877), accuracy(train) = 0.9668, accuracy(test) = 0.9296\n",
      "Epoch 36 loss(train) = variable(6.138486862182617), accuracy(train) = 0.9682, accuracy(test) = 0.9312\n",
      "Epoch 37 loss(train) = variable(5.962985038757324), accuracy(train) = 0.9688, accuracy(test) = 0.9326\n",
      "Epoch 38 loss(train) = variable(5.795360565185547), accuracy(train) = 0.9698, accuracy(test) = 0.9336\n",
      "Epoch 39 loss(train) = variable(5.633546829223633), accuracy(train) = 0.971, accuracy(test) = 0.9344\n",
      "Epoch 40 loss(train) = variable(5.476174831390381), accuracy(train) = 0.9712, accuracy(test) = 0.936\n",
      "Epoch 41 loss(train) = variable(5.322971343994141), accuracy(train) = 0.9722, accuracy(test) = 0.9374\n",
      "Epoch 42 loss(train) = variable(5.172061443328857), accuracy(train) = 0.9734, accuracy(test) = 0.9376\n",
      "Epoch 43 loss(train) = variable(5.023708820343018), accuracy(train) = 0.9746, accuracy(test) = 0.9376\n",
      "Epoch 44 loss(train) = variable(4.877342224121094), accuracy(train) = 0.975, accuracy(test) = 0.939\n",
      "Epoch 45 loss(train) = variable(4.732467174530029), accuracy(train) = 0.9766, accuracy(test) = 0.9402\n",
      "Epoch 46 loss(train) = variable(4.589151382446289), accuracy(train) = 0.9768, accuracy(test) = 0.9406\n",
      "Epoch 47 loss(train) = variable(4.447197914123535), accuracy(train) = 0.9772, accuracy(test) = 0.9408\n",
      "Epoch 48 loss(train) = variable(4.306910514831543), accuracy(train) = 0.9776, accuracy(test) = 0.9416\n",
      "Epoch 49 loss(train) = variable(4.167518138885498), accuracy(train) = 0.9784, accuracy(test) = 0.943\n",
      "Epoch 50 loss(train) = variable(4.030010223388672), accuracy(train) = 0.979, accuracy(test) = 0.9442\n",
      "Epoch 51 loss(train) = variable(3.894306182861328), accuracy(train) = 0.9792, accuracy(test) = 0.9446\n",
      "Epoch 52 loss(train) = variable(3.7621617317199707), accuracy(train) = 0.9796, accuracy(test) = 0.9454\n",
      "Epoch 53 loss(train) = variable(3.6340889930725098), accuracy(train) = 0.9798, accuracy(test) = 0.9454\n",
      "Epoch 54 loss(train) = variable(3.509894609451294), accuracy(train) = 0.9804, accuracy(test) = 0.9464\n",
      "Epoch 55 loss(train) = variable(3.390115737915039), accuracy(train) = 0.9816, accuracy(test) = 0.9472\n",
      "Epoch 56 loss(train) = variable(3.2749199867248535), accuracy(train) = 0.982, accuracy(test) = 0.9478\n",
      "Epoch 57 loss(train) = variable(3.1638689041137695), accuracy(train) = 0.983, accuracy(test) = 0.9488\n",
      "Epoch 58 loss(train) = variable(3.0568883419036865), accuracy(train) = 0.9836, accuracy(test) = 0.9492\n",
      "Epoch 59 loss(train) = variable(2.954360008239746), accuracy(train) = 0.984, accuracy(test) = 0.9506\n",
      "Epoch 60 loss(train) = variable(2.8551347255706787), accuracy(train) = 0.9858, accuracy(test) = 0.9506\n",
      "Epoch 61 loss(train) = variable(2.759455680847168), accuracy(train) = 0.9862, accuracy(test) = 0.9512\n",
      "Epoch 62 loss(train) = variable(2.6673638820648193), accuracy(train) = 0.9864, accuracy(test) = 0.9518\n",
      "Epoch 63 loss(train) = variable(2.578274726867676), accuracy(train) = 0.987, accuracy(test) = 0.9522\n",
      "Epoch 64 loss(train) = variable(2.492264986038208), accuracy(train) = 0.9872, accuracy(test) = 0.9524\n",
      "Epoch 65 loss(train) = variable(2.40862774848938), accuracy(train) = 0.9876, accuracy(test) = 0.9528\n",
      "Epoch 66 loss(train) = variable(2.3274903297424316), accuracy(train) = 0.988, accuracy(test) = 0.9534\n",
      "Epoch 67 loss(train) = variable(2.2491257190704346), accuracy(train) = 0.9884, accuracy(test) = 0.954\n",
      "Epoch 68 loss(train) = variable(2.17275071144104), accuracy(train) = 0.989, accuracy(test) = 0.9546\n",
      "Epoch 69 loss(train) = variable(2.0987548828125), accuracy(train) = 0.9902, accuracy(test) = 0.9548\n",
      "Epoch 70 loss(train) = variable(2.02713942527771), accuracy(train) = 0.9906, accuracy(test) = 0.9554\n",
      "Epoch 71 loss(train) = variable(1.9577600955963135), accuracy(train) = 0.991, accuracy(test) = 0.955\n",
      "Epoch 72 loss(train) = variable(1.8902642726898193), accuracy(train) = 0.9922, accuracy(test) = 0.9546\n",
      "Epoch 73 loss(train) = variable(1.8245463371276855), accuracy(train) = 0.9924, accuracy(test) = 0.9548\n",
      "Epoch 74 loss(train) = variable(1.7610596418380737), accuracy(train) = 0.9932, accuracy(test) = 0.9552\n",
      "Epoch 75 loss(train) = variable(1.699537754058838), accuracy(train) = 0.9932, accuracy(test) = 0.9554\n",
      "Epoch 76 loss(train) = variable(1.6399601697921753), accuracy(train) = 0.9938, accuracy(test) = 0.9556\n",
      "Epoch 77 loss(train) = variable(1.5820467472076416), accuracy(train) = 0.9938, accuracy(test) = 0.9558\n",
      "Epoch 78 loss(train) = variable(1.5260050296783447), accuracy(train) = 0.9942, accuracy(test) = 0.9562\n",
      "Epoch 79 loss(train) = variable(1.4717336893081665), accuracy(train) = 0.9946, accuracy(test) = 0.9568\n",
      "Epoch 80 loss(train) = variable(1.419180154800415), accuracy(train) = 0.9948, accuracy(test) = 0.9568\n",
      "Epoch 81 loss(train) = variable(1.3680119514465332), accuracy(train) = 0.995, accuracy(test) = 0.9572\n",
      "Epoch 82 loss(train) = variable(1.3181889057159424), accuracy(train) = 0.9956, accuracy(test) = 0.9574\n",
      "Epoch 83 loss(train) = variable(1.2699905633926392), accuracy(train) = 0.9956, accuracy(test) = 0.9582\n",
      "Epoch 84 loss(train) = variable(1.2231879234313965), accuracy(train) = 0.996, accuracy(test) = 0.9582\n",
      "Epoch 85 loss(train) = variable(1.1773042678833008), accuracy(train) = 0.9964, accuracy(test) = 0.9584\n",
      "Epoch 86 loss(train) = variable(1.1333223581314087), accuracy(train) = 0.9964, accuracy(test) = 0.9584\n",
      "Epoch 87 loss(train) = variable(1.090407371520996), accuracy(train) = 0.997, accuracy(test) = 0.9586\n",
      "Epoch 88 loss(train) = variable(1.0489202737808228), accuracy(train) = 0.997, accuracy(test) = 0.959\n",
      "Epoch 89 loss(train) = variable(1.008574366569519), accuracy(train) = 0.9976, accuracy(test) = 0.959\n",
      "Epoch 90 loss(train) = variable(0.9694769382476807), accuracy(train) = 0.9976, accuracy(test) = 0.9594\n",
      "Epoch 91 loss(train) = variable(0.9316802024841309), accuracy(train) = 0.9978, accuracy(test) = 0.96\n",
      "Epoch 92 loss(train) = variable(0.8950104713439941), accuracy(train) = 0.9978, accuracy(test) = 0.96\n",
      "Epoch 93 loss(train) = variable(0.8596038818359375), accuracy(train) = 0.9978, accuracy(test) = 0.9602\n",
      "Epoch 94 loss(train) = variable(0.8255934119224548), accuracy(train) = 0.998, accuracy(test) = 0.9606\n",
      "Epoch 95 loss(train) = variable(0.7928503155708313), accuracy(train) = 0.998, accuracy(test) = 0.9608\n",
      "Epoch 96 loss(train) = variable(0.7613893747329712), accuracy(train) = 0.998, accuracy(test) = 0.9606\n",
      "Epoch 97 loss(train) = variable(0.7312126159667969), accuracy(train) = 0.998, accuracy(test) = 0.9608\n",
      "Epoch 98 loss(train) = variable(0.7026003003120422), accuracy(train) = 0.998, accuracy(test) = 0.9612\n",
      "Epoch 99 loss(train) = variable(0.6753947138786316), accuracy(train) = 0.998, accuracy(test) = 0.9616\n",
      "Epoch 100 loss(train) = variable(0.6494655609130859), accuracy(train) = 0.9982, accuracy(test) = 0.9614\n"
     ]
    }
   ],
   "source": [
    "# learn\n",
    "\n",
    "model = LeNet(10)\n",
    "#optimizer = optimizers.SGD()\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "batchsize = 100\n",
    "datasize = len(xs)\n",
    "\n",
    "# use GPU\n",
    "chainer.cuda.get_device_from_id(0).use()\n",
    "model.to_gpu()\n",
    "\n",
    "xp = cuda.cupy\n",
    "\n",
    "# output file\n",
    "f = open(\"n{0}.dat\".format(datasize), mode='w')\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i in range(0, datasize, batchsize):\n",
    "        x = xs[i:i + batchsize]\n",
    "        t = ts[i:i + batchsize]\n",
    "        \n",
    "        var_x = Variable(cuda.to_gpu(x))\n",
    "        #var_x = Variable(x)\n",
    "        t = Variable(cuda.to_gpu(np.array(t, \"i\")))\n",
    "        #t = Variable(np.array(t, \"i\"))\n",
    "        y = model(var_x)\n",
    "        \n",
    "        model.cleargrads()\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "    accuracy_train, loss_train = check_accuracy(model, xs, ts, batchsize)\n",
    "    accuracy_test, _           = check_accuracy(model, txs, tts, batchsize)\n",
    "    \n",
    "    optimizer.new_epoch()\n",
    "    \n",
    "    #print(\"1: weight={0}, bias={1}\".format(model.conv1.W, model.conv1.b))\n",
    "    print(\"Epoch {0} loss(train) = {1}, accuracy(train) = {2}, accuracy(test) = {3}\".format(epoch + 1, loss_train, accuracy_train, accuracy_test))\n",
    "    f.write(\"{0} {1}\\n\".format(epoch, accuracy_test))\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
