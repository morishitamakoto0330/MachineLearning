{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LeNet Class\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import optimizers\n",
    "from chainer import Variable\n",
    "from chainer import cuda\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LeNet(chainer.Chain):\n",
    "    def __init__(self, num_class, train=True):\n",
    "        super(LeNet, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1=L.Convolution2D(None, 6, 5, stride=1)\n",
    "            self.conv2=L.Convolution2D(None, 16, 5, stride=1)\n",
    "            self.conv3=L.Convolution2D(None, 120, 5, stride=1)\n",
    "            self.fc4=L.Linear(None, 84)\n",
    "            self.fc5=L.Linear(None, num_class)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        h1 = F.max_pooling_2d(F.local_response_normalization(\n",
    "            F.sigmoid(self.conv1(x))), 2, stride=2)\n",
    "        h2 = F.max_pooling_2d(F.local_response_normalization(\n",
    "            F.sigmoid(self.conv2(h1))), 2, stride=2)\n",
    "        h3 = F.sigmoid(self.conv3(h2))\n",
    "        h4 = F.sigmoid(self.fc4(h3))\n",
    "        h5 = self.fc5(h4)\n",
    "        \n",
    "        return h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MNIST data set\n",
    "\n",
    "train, test = chainer.datasets.get_mnist()\n",
    "_xs, ts = train._datasets\n",
    "_txs, tts = test._datasets\n",
    "\n",
    "size = 1000\n",
    "_xs = _xs[:size]\n",
    "ts = ts[:size]\n",
    "_txs = _txs[:size]\n",
    "tts = tts[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# padding (60000, 784) -> (60000, 1, 28, 28) -> (60000, 1, 32, 32)\n",
    "\n",
    "_v0 = np.row_stack((np.zeros(28), np.zeros(28)))\n",
    "v0 = np.array(_v0)\n",
    "_h0 = np.column_stack((np.zeros(32), np.zeros(32)))\n",
    "h0 = np.array(_h0)\n",
    "\n",
    "def padding(x):\n",
    "    tmp1 = np.vstack((x, v0))\n",
    "    tmp2 = np.vstack((v0, tmp1))\n",
    "    _tmp1 = np.hstack((tmp2, h0))\n",
    "    _tmp2 = np.hstack((h0, _tmp1))\n",
    "    return _tmp2\n",
    "\n",
    "xs_list = []\n",
    "for i in range(len(_xs)):\n",
    "    x = np.reshape(_xs[i], (28, 28))\n",
    "    pad_x = padding(x)\n",
    "    xs_list.append(pad_x[np.newaxis, :, :])\n",
    "txs_list = []\n",
    "for i in range(len(_txs)):\n",
    "    tx = np.reshape(_txs[i], (28, 28))\n",
    "    pad_tx = padding(tx)\n",
    "    txs_list.append(pad_tx[np.newaxis, :, :])\n",
    "    \n",
    "xs = np.array(xs_list, dtype=np.float32)\n",
    "txs = np.array(txs_list, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# method\n",
    "\n",
    "def check_accuracy(model, xs, ts, batchsize):\n",
    "    loss = 0\n",
    "    num_cors = 0\n",
    "    for i in range(0, len(xs), batchsize):\n",
    "        x = xs[i:i + batchsize]\n",
    "        t = ts[i:i + batchsize]\n",
    "        \n",
    "        var_xs = Variable(cuda.to_gpu(x))\n",
    "        #var_xs = Variable(x)\n",
    "        t = Variable(cuda.to_gpu(np.array(t, \"i\")))\n",
    "        #t = Variable(np.array(t, \"i\"))\n",
    "        ys = model(var_xs)\n",
    "    \n",
    "        loss += F.softmax_cross_entropy(ys, t)\n",
    "        ys = np.argmax(ys.data, axis=1)\n",
    "        _t = cuda.to_gpu(np.array(cuda.to_cpu(t.data), dtype=np.float32))\n",
    "        #_t = np.array(t.data, dtype=np.float32)\n",
    "        cors = (ys == _t)\n",
    "        num_cors += sum(cors)\n",
    "    accuracy = num_cors / ts.shape[0]\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss(train) = variable(23.01917839050293), accuracy(train) = 0.105, accuracy(test) = 0.11\n",
      "Epoch 2 loss(train) = variable(22.993305206298828), accuracy(train) = 0.117, accuracy(test) = 0.099\n",
      "Epoch 3 loss(train) = variable(22.98738670349121), accuracy(train) = 0.116, accuracy(test) = 0.126\n",
      "Epoch 4 loss(train) = variable(22.981300354003906), accuracy(train) = 0.117, accuracy(test) = 0.099\n",
      "Epoch 5 loss(train) = variable(22.976879119873047), accuracy(train) = 0.117, accuracy(test) = 0.099\n",
      "Epoch 6 loss(train) = variable(22.97380828857422), accuracy(train) = 0.117, accuracy(test) = 0.099\n",
      "Epoch 7 loss(train) = variable(22.970569610595703), accuracy(train) = 0.117, accuracy(test) = 0.099\n",
      "Epoch 8 loss(train) = variable(22.964231491088867), accuracy(train) = 0.117, accuracy(test) = 0.099\n",
      "Epoch 9 loss(train) = variable(22.954526901245117), accuracy(train) = 0.117, accuracy(test) = 0.099\n",
      "Epoch 10 loss(train) = variable(22.938983917236328), accuracy(train) = 0.117, accuracy(test) = 0.099\n",
      "Epoch 11 loss(train) = variable(22.912822723388672), accuracy(train) = 0.117, accuracy(test) = 0.099\n",
      "Epoch 12 loss(train) = variable(22.8689022064209), accuracy(train) = 0.117, accuracy(test) = 0.1\n",
      "Epoch 13 loss(train) = variable(22.794944763183594), accuracy(train) = 0.221, accuracy(test) = 0.211\n",
      "Epoch 14 loss(train) = variable(22.669151306152344), accuracy(train) = 0.227, accuracy(test) = 0.218\n",
      "Epoch 15 loss(train) = variable(22.453147888183594), accuracy(train) = 0.268, accuracy(test) = 0.255\n",
      "Epoch 16 loss(train) = variable(22.081256866455078), accuracy(train) = 0.382, accuracy(test) = 0.346\n",
      "Epoch 17 loss(train) = variable(21.45392417907715), accuracy(train) = 0.522, accuracy(test) = 0.465\n",
      "Epoch 18 loss(train) = variable(20.462800979614258), accuracy(train) = 0.547, accuracy(test) = 0.527\n",
      "Epoch 19 loss(train) = variable(19.077320098876953), accuracy(train) = 0.545, accuracy(test) = 0.53\n",
      "Epoch 20 loss(train) = variable(17.435535430908203), accuracy(train) = 0.553, accuracy(test) = 0.555\n",
      "Epoch 21 loss(train) = variable(15.781846046447754), accuracy(train) = 0.587, accuracy(test) = 0.582\n",
      "Epoch 22 loss(train) = variable(14.299172401428223), accuracy(train) = 0.629, accuracy(test) = 0.61\n",
      "Epoch 23 loss(train) = variable(13.047953605651855), accuracy(train) = 0.672, accuracy(test) = 0.657\n",
      "Epoch 24 loss(train) = variable(12.008296012878418), accuracy(train) = 0.705, accuracy(test) = 0.692\n",
      "Epoch 25 loss(train) = variable(11.131623268127441), accuracy(train) = 0.747, accuracy(test) = 0.727\n",
      "Epoch 26 loss(train) = variable(10.372305870056152), accuracy(train) = 0.765, accuracy(test) = 0.747\n",
      "Epoch 27 loss(train) = variable(9.698690414428711), accuracy(train) = 0.781, accuracy(test) = 0.752\n",
      "Epoch 28 loss(train) = variable(9.092105865478516), accuracy(train) = 0.802, accuracy(test) = 0.76\n",
      "Epoch 29 loss(train) = variable(8.542116165161133), accuracy(train) = 0.811, accuracy(test) = 0.77\n",
      "Epoch 30 loss(train) = variable(8.042158126831055), accuracy(train) = 0.815, accuracy(test) = 0.774\n",
      "Epoch 31 loss(train) = variable(7.587405681610107), accuracy(train) = 0.827, accuracy(test) = 0.784\n",
      "Epoch 32 loss(train) = variable(7.173478126525879), accuracy(train) = 0.833, accuracy(test) = 0.79\n",
      "Epoch 33 loss(train) = variable(6.7961249351501465), accuracy(train) = 0.84, accuracy(test) = 0.797\n",
      "Epoch 34 loss(train) = variable(6.451016426086426), accuracy(train) = 0.848, accuracy(test) = 0.803\n",
      "Epoch 35 loss(train) = variable(6.134210109710693), accuracy(train) = 0.848, accuracy(test) = 0.806\n",
      "Epoch 36 loss(train) = variable(5.842206954956055), accuracy(train) = 0.855, accuracy(test) = 0.807\n",
      "Epoch 37 loss(train) = variable(5.571843147277832), accuracy(train) = 0.868, accuracy(test) = 0.813\n",
      "Epoch 38 loss(train) = variable(5.320677757263184), accuracy(train) = 0.877, accuracy(test) = 0.816\n",
      "Epoch 39 loss(train) = variable(5.086366653442383), accuracy(train) = 0.879, accuracy(test) = 0.817\n",
      "Epoch 40 loss(train) = variable(4.867059230804443), accuracy(train) = 0.888, accuracy(test) = 0.822\n",
      "Epoch 41 loss(train) = variable(4.661252021789551), accuracy(train) = 0.892, accuracy(test) = 0.825\n",
      "Epoch 42 loss(train) = variable(4.467692852020264), accuracy(train) = 0.897, accuracy(test) = 0.835\n",
      "Epoch 43 loss(train) = variable(4.285334587097168), accuracy(train) = 0.9, accuracy(test) = 0.838\n",
      "Epoch 44 loss(train) = variable(4.113418102264404), accuracy(train) = 0.906, accuracy(test) = 0.844\n",
      "Epoch 45 loss(train) = variable(3.9512104988098145), accuracy(train) = 0.908, accuracy(test) = 0.848\n",
      "Epoch 46 loss(train) = variable(3.7981297969818115), accuracy(train) = 0.912, accuracy(test) = 0.853\n",
      "Epoch 47 loss(train) = variable(3.653592586517334), accuracy(train) = 0.915, accuracy(test) = 0.857\n",
      "Epoch 48 loss(train) = variable(3.517038583755493), accuracy(train) = 0.92, accuracy(test) = 0.863\n",
      "Epoch 49 loss(train) = variable(3.3879544734954834), accuracy(train) = 0.925, accuracy(test) = 0.865\n",
      "Epoch 50 loss(train) = variable(3.265953779220581), accuracy(train) = 0.928, accuracy(test) = 0.866\n",
      "Epoch 51 loss(train) = variable(3.150677442550659), accuracy(train) = 0.933, accuracy(test) = 0.868\n",
      "Epoch 52 loss(train) = variable(3.0416815280914307), accuracy(train) = 0.936, accuracy(test) = 0.869\n",
      "Epoch 53 loss(train) = variable(2.9385931491851807), accuracy(train) = 0.939, accuracy(test) = 0.871\n",
      "Epoch 54 loss(train) = variable(2.840977191925049), accuracy(train) = 0.943, accuracy(test) = 0.87\n",
      "Epoch 55 loss(train) = variable(2.7484965324401855), accuracy(train) = 0.944, accuracy(test) = 0.872\n",
      "Epoch 56 loss(train) = variable(2.660827159881592), accuracy(train) = 0.945, accuracy(test) = 0.874\n",
      "Epoch 57 loss(train) = variable(2.577735185623169), accuracy(train) = 0.946, accuracy(test) = 0.874\n",
      "Epoch 58 loss(train) = variable(2.4988467693328857), accuracy(train) = 0.945, accuracy(test) = 0.876\n",
      "Epoch 59 loss(train) = variable(2.4238576889038086), accuracy(train) = 0.948, accuracy(test) = 0.88\n",
      "Epoch 60 loss(train) = variable(2.352449655532837), accuracy(train) = 0.948, accuracy(test) = 0.881\n",
      "Epoch 61 loss(train) = variable(2.284346580505371), accuracy(train) = 0.949, accuracy(test) = 0.883\n",
      "Epoch 62 loss(train) = variable(2.2193076610565186), accuracy(train) = 0.951, accuracy(test) = 0.883\n",
      "Epoch 63 loss(train) = variable(2.1571552753448486), accuracy(train) = 0.952, accuracy(test) = 0.884\n",
      "Epoch 64 loss(train) = variable(2.0976991653442383), accuracy(train) = 0.956, accuracy(test) = 0.883\n",
      "Epoch 65 loss(train) = variable(2.0407724380493164), accuracy(train) = 0.958, accuracy(test) = 0.883\n",
      "Epoch 66 loss(train) = variable(1.9861847162246704), accuracy(train) = 0.958, accuracy(test) = 0.883\n",
      "Epoch 67 loss(train) = variable(1.9337952136993408), accuracy(train) = 0.958, accuracy(test) = 0.886\n",
      "Epoch 68 loss(train) = variable(1.8834762573242188), accuracy(train) = 0.958, accuracy(test) = 0.886\n",
      "Epoch 69 loss(train) = variable(1.835079312324524), accuracy(train) = 0.96, accuracy(test) = 0.887\n",
      "Epoch 70 loss(train) = variable(1.7885022163391113), accuracy(train) = 0.961, accuracy(test) = 0.887\n",
      "Epoch 71 loss(train) = variable(1.7436503171920776), accuracy(train) = 0.963, accuracy(test) = 0.887\n",
      "Epoch 72 loss(train) = variable(1.7004129886627197), accuracy(train) = 0.963, accuracy(test) = 0.889\n",
      "Epoch 73 loss(train) = variable(1.6586979627609253), accuracy(train) = 0.964, accuracy(test) = 0.889\n",
      "Epoch 74 loss(train) = variable(1.618411660194397), accuracy(train) = 0.965, accuracy(test) = 0.889\n",
      "Epoch 75 loss(train) = variable(1.5794799327850342), accuracy(train) = 0.965, accuracy(test) = 0.888\n",
      "Epoch 76 loss(train) = variable(1.541824221611023), accuracy(train) = 0.965, accuracy(test) = 0.89\n",
      "Epoch 77 loss(train) = variable(1.5053569078445435), accuracy(train) = 0.966, accuracy(test) = 0.891\n",
      "Epoch 78 loss(train) = variable(1.4700335264205933), accuracy(train) = 0.968, accuracy(test) = 0.892\n",
      "Epoch 79 loss(train) = variable(1.4358187913894653), accuracy(train) = 0.97, accuracy(test) = 0.892\n",
      "Epoch 80 loss(train) = variable(1.40264093875885), accuracy(train) = 0.971, accuracy(test) = 0.892\n",
      "Epoch 81 loss(train) = variable(1.3704369068145752), accuracy(train) = 0.971, accuracy(test) = 0.892\n",
      "Epoch 82 loss(train) = variable(1.3391469717025757), accuracy(train) = 0.971, accuracy(test) = 0.893\n",
      "Epoch 83 loss(train) = variable(1.308732509613037), accuracy(train) = 0.971, accuracy(test) = 0.896\n",
      "Epoch 84 loss(train) = variable(1.2791764736175537), accuracy(train) = 0.974, accuracy(test) = 0.897\n",
      "Epoch 85 loss(train) = variable(1.250428557395935), accuracy(train) = 0.976, accuracy(test) = 0.896\n",
      "Epoch 86 loss(train) = variable(1.2224624156951904), accuracy(train) = 0.976, accuracy(test) = 0.894\n",
      "Epoch 87 loss(train) = variable(1.1952345371246338), accuracy(train) = 0.979, accuracy(test) = 0.894\n",
      "Epoch 88 loss(train) = variable(1.1687219142913818), accuracy(train) = 0.979, accuracy(test) = 0.897\n",
      "Epoch 89 loss(train) = variable(1.1428941488265991), accuracy(train) = 0.98, accuracy(test) = 0.897\n",
      "Epoch 90 loss(train) = variable(1.1177247762680054), accuracy(train) = 0.98, accuracy(test) = 0.897\n",
      "Epoch 91 loss(train) = variable(1.0931808948516846), accuracy(train) = 0.981, accuracy(test) = 0.899\n",
      "Epoch 92 loss(train) = variable(1.0692425966262817), accuracy(train) = 0.983, accuracy(test) = 0.9\n",
      "Epoch 93 loss(train) = variable(1.0458824634552002), accuracy(train) = 0.985, accuracy(test) = 0.9\n",
      "Epoch 94 loss(train) = variable(1.023092269897461), accuracy(train) = 0.985, accuracy(test) = 0.9\n",
      "Epoch 95 loss(train) = variable(1.0008503198623657), accuracy(train) = 0.985, accuracy(test) = 0.9\n",
      "Epoch 96 loss(train) = variable(0.9791450500488281), accuracy(train) = 0.985, accuracy(test) = 0.9\n",
      "Epoch 97 loss(train) = variable(0.957959771156311), accuracy(train) = 0.985, accuracy(test) = 0.902\n",
      "Epoch 98 loss(train) = variable(0.9372841715812683), accuracy(train) = 0.985, accuracy(test) = 0.902\n",
      "Epoch 99 loss(train) = variable(0.9171067476272583), accuracy(train) = 0.985, accuracy(test) = 0.902\n",
      "Epoch 100 loss(train) = variable(0.8974116444587708), accuracy(train) = 0.985, accuracy(test) = 0.901\n"
     ]
    }
   ],
   "source": [
    "# learn\n",
    "\n",
    "model = LeNet(10)\n",
    "#optimizer = optimizers.SGD()\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "batchsize = 100\n",
    "datasize = len(xs)\n",
    "\n",
    "# use GPU\n",
    "chainer.cuda.get_device_from_id(0).use()\n",
    "model.to_gpu()\n",
    "\n",
    "xp = cuda.cupy\n",
    "\n",
    "# output file\n",
    "f = open(\"n{0}.dat\".format(datasize), mode='w')\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i in range(0, datasize, batchsize):\n",
    "        x = xs[i:i + batchsize]\n",
    "        t = ts[i:i + batchsize]\n",
    "        \n",
    "        var_x = Variable(cuda.to_gpu(x))\n",
    "        #var_x = Variable(x)\n",
    "        t = Variable(cuda.to_gpu(np.array(t, \"i\")))\n",
    "        #t = Variable(np.array(t, \"i\"))\n",
    "        y = model(var_x)\n",
    "        \n",
    "        model.cleargrads()\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "    accuracy_train, loss_train = check_accuracy(model, xs, ts, batchsize)\n",
    "    accuracy_test, _           = check_accuracy(model, txs, tts, batchsize)\n",
    "    \n",
    "    optimizer.new_epoch()\n",
    "    \n",
    "    #print(\"1: weight={0}, bias={1}\".format(model.conv1.W, model.conv1.b))\n",
    "    print(\"Epoch {0} loss(train) = {1}, accuracy(train) = {2}, accuracy(test) = {3}\".format(epoch + 1, loss_train, accuracy_train, accuracy_test))\n",
    "    f.write(\"{0} {1}\\n\".format(epoch, accuracy_test))\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
